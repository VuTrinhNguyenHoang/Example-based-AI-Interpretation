{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "# Release memory\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b977d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Create train, validation, and test splits\n",
    "train_dataset = dataset['train'].select(range(40000))\n",
    "valid_dataset = dataset['train'].select(range(40000, 45000))\n",
    "test_dataset = dataset['test'].select(range(5000))\n",
    "\n",
    "# train_df = pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv')\n",
    "# valid_df = pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Valid.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv')\n",
    "\n",
    "# train_df = train_df.head(10)\n",
    "# valid_df = valid_df.head(2)\n",
    "# test_df = test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape, valid_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize texts\n",
    "def tokenize_texts(texts, max_length=256):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# X_train_texts = train_df['text'].tolist()\n",
    "# X_valid_texts = valid_df['text'].tolist()\n",
    "# X_test_texts = test_df['text'].tolist()\n",
    "X_train_texts = [item['text'] for item in train_dataset]\n",
    "X_valid_texts = [item['text'] for item in valid_dataset]\n",
    "X_test_texts = [item['text'] for item in test_dataset]\n",
    "train_encodings = tokenize_texts(X_train_texts)\n",
    "valid_encodings = tokenize_texts(X_valid_texts)\n",
    "test_encodings = tokenize_texts(X_test_texts)\n",
    "\n",
    "# train_labels = tf.convert_to_tensor(train_df['label'].values, dtype=tf.int32)\n",
    "# valid_labels = tf.convert_to_tensor(valid_df['label'].values, dtype=tf.int32)\n",
    "# test_labels = tf.convert_to_tensor(test_df['label'].values, dtype=tf.int32)\n",
    "train_labels = tf.convert_to_tensor([item['label'] for item in train_dataset], dtype=tf.int32)\n",
    "valid_labels = tf.convert_to_tensor([item['label'] for item in valid_dataset], dtype=tf.int32)\n",
    "test_labels = tf.convert_to_tensor([item['label'] for item in test_dataset], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73731c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = tf.keras.layers.Dense(4, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(cls_output)\n",
    "        \n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_classifier = BertClassifier(bert_model)\n",
    "\n",
    "bert_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(valid_encodings),\n",
    "    valid_labels\n",
    "))\n",
    "\n",
    "# Batch the datasets\n",
    "batch_size = 8  # Adjust based on your hardware\n",
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bert_classifier.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training history:\", history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7825505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_c_hp(texts, class_indices, bert_classifier, tokenizer, batch_size=8):\n",
    "    bert_model = bert_classifier.bert\n",
    "    embeddings_layer = bert_model.get_input_embeddings()\n",
    "\n",
    "    c_hp_features = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, num_texts, batch_size), desc=\"Computing C-HP features\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_indices = class_indices[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='tf',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256  # Reduced to match training\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Convert input_ids to embeddings\n",
    "        embeddings = embeddings_layer(input_ids)\n",
    "        embeddings = tf.cast(embeddings, tf.float32)\n",
    "\n",
    "        # Use GradientTape to compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(embeddings)\n",
    "            # Forward pass with inputs_embeds\n",
    "            outputs = bert_model({\n",
    "                \"inputs_embeds\": embeddings,\n",
    "                \"attention_mask\": attention_mask\n",
    "            })\n",
    "            cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            preds = bert_classifier.classifier(cls_output)\n",
    "            # Compute loss for the specified class_idx for the entire batch\n",
    "            batch_indices_tensor = tf.constant(batch_indices, dtype=tf.int32)\n",
    "            batch_indices_range = tf.range(tf.shape(preds)[0])\n",
    "            selected_preds = tf.gather_nd(preds, tf.stack([batch_indices_range, batch_indices_tensor], axis=1))\n",
    "            loss = tf.reduce_mean(selected_preds)\n",
    "\n",
    "        # Compute gradients with respect to [CLS] output\n",
    "        grads = tape.gradient(loss, cls_output)\n",
    "        if grads is None:\n",
    "            raise ValueError(\"Gradients is None. Check if the model is trainable or if cls_output is properly watched.\")\n",
    "\n",
    "        # Compute C-HP: Element-wise multiplication of gradients and [CLS] embedding\n",
    "        c_hp = grads * cls_output\n",
    "        c_hp_flat = tf.reshape(c_hp, [tf.shape(c_hp)[0], -1])\n",
    "        c_hp_features.append(c_hp_flat.numpy())\n",
    "\n",
    "    # Stack all features into a single array\n",
    "    return np.vstack(c_hp_features)\n",
    "\n",
    "def compute_raw_features(texts, class_indices, bert_classifier, tokenizer, batch_size=8):\n",
    "    bert_model = bert_classifier.bert\n",
    "    embeddings_layer = bert_model.get_input_embeddings()\n",
    "\n",
    "    c_hp_features = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, num_texts, batch_size), desc=\"Computing C-HP features\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_indices = class_indices[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='tf',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256  # Reduced to match training\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Convert input_ids to embeddings\n",
    "        embeddings = embeddings_layer(input_ids)\n",
    "        embeddings = tf.cast(embeddings, tf.float32)\n",
    "\n",
    "        # Use GradientTape to compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(embeddings)\n",
    "            # Forward pass with inputs_embeds\n",
    "            outputs = bert_model({\n",
    "                \"inputs_embeds\": embeddings,\n",
    "                \"attention_mask\": attention_mask\n",
    "            })\n",
    "            cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            \n",
    "        c_hp = cls_output\n",
    "        c_hp_flat = tf.reshape(c_hp, [tf.shape(c_hp)[0], -1])\n",
    "        c_hp_features.append(c_hp_flat.numpy())\n",
    "\n",
    "    # Stack all features into a single array\n",
    "    return np.vstack(c_hp_features)\n",
    "\n",
    "def predict_with_bert(bert_classifier, texts, tokenizer, batch_size=8):\n",
    "    predictions = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, num_texts, batch_size), desc=\"Predicting with BERT\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='tf',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "        # Predict for current batch\n",
    "        batch_predictions = bert_classifier(dict(inputs))\n",
    "        batch_labels = np.argmax(batch_predictions, axis=1)\n",
    "        predictions.append(batch_labels)\n",
    "\n",
    "    # Concatenate all predictions into a single array\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test_pred = predict_with_bert(bert_classifier, X_test_texts, tokenizer)\n",
    "\n",
    "print('acc', accuracy_score(test_labels, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf105961",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict_with_bert(bert_classifier, X_train_texts, tokenizer)\n",
    "c_hp_features_train = compute_c_hp(\n",
    "    texts=X_train_texts,\n",
    "    class_indices=y_train_pred,\n",
    "    bert_classifier=bert_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "c_hp_features_test = compute_c_hp(\n",
    "    texts=X_test_texts,\n",
    "    class_indices=y_test_pred,\n",
    "    bert_classifier=bert_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(c_hp_features_train, y_train_pred)\n",
    "y_knn_pred = knn.predict(c_hp_features_test)\n",
    "\n",
    "print('acc', accuracy_score(test_labels, y_knn_pred))\n",
    "print('agreement:', accuracy_score(y_test_pred, y_knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "sample_text = X_test_texts[id]\n",
    "sample_classIdx = y_test_pred[id]\n",
    "true_label = test_labels[id].numpy()\n",
    "\n",
    "# sample_classIdx, true_label\n",
    "\n",
    "sample_chp = compute_c_hp(\n",
    "    texts=[sample_text],\n",
    "    class_indices=[sample_classIdx],\n",
    "    bert_classifier=bert_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(sample_chp)\n",
    "\n",
    "print(f\"\\nTest sample: {sample_text}\")\n",
    "print(f\"Prediction: {knn.predict(sample_chp)[0]}\")\n",
    "print(\"Similar cases (indices):\", indices[0])\n",
    "print(\"Distances:\", distances[0])\n",
    "print(\"\\nSimilar texts:\")\n",
    "for idx in indices[0]:\n",
    "    print(f\"- {X_train_texts[idx]} (Label: {train_labels[idx]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c80991",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features_train = compute_raw_features(\n",
    "    texts=X_train_texts,\n",
    "    class_indices=y_train_pred,\n",
    "    bert_classifier=bert_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "raw_features_test = compute_raw_features(\n",
    "    texts=X_test_texts,\n",
    "    class_indices=y_test_pred,\n",
    "    bert_classifier=bert_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(raw_features_train, y_train_pred)\n",
    "y_knn_pred = knn.predict(raw_features_test)\n",
    "\n",
    "print('acc', accuracy_score(test_labels, y_knn_pred))\n",
    "print('agreement:', accuracy_score(y_test_pred, y_knn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
